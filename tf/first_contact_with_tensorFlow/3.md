# 3. TensorFlow 中的聚类

前一章中介绍的线性回归是一种监督学习算法，我们使用数据和输出值（或标签）来构建适合它们的模型。但我们并不总是拥有标记数据，尽管如此，我们也希望以某种方式分析它们。在这种情况下，我们可以使用无监督学习算法，例如聚类。聚类方法被广泛使用，因为它通常是数据分析的初步筛选的好方法。

在本章中，我将介绍名为 K-means 的聚类算法。它肯定是最受欢迎的，广泛用于自动将数据分组到相关的子集中，以便子集中的所有元素彼此更相似。在此算法中，我们没有任何目标或结果变量来预测估计值。

我还将使用本章来介绍 TensorFlow 的知识，并在更详细地介绍名为`tensor`（张量）的基本数据结构。我将首先解释这种类型的数据是什么样的，并展示可以在其上执行的转换。然后，我将使用张量在案例研究中展示 K-means 算法的使用。

### 基本数据结构：张量

TensorFlow 程序使用称为张量的基本数据结构来表示其所有数据。张量可以被认为是动态大小的多维数据数组，其具有静态数据类型的属性，可以从布尔值或字符串到各种数字类型。下面是 Python 中的主要类型及其等价物的表格。


| TensorFlow 中的类型 | Python 中的类型 | 描述 |
| --- | --- | --- |
| `DT_FLOAT` | `tf.float32` | 32 位浮点 |
| `DT_INT16` | `tf.int16` | 16 位整数 |
| `DT_INT32` | `tf.int32` | 32 位整数 |
| `DT_INT64` | `tf.int64` | 64 位整数 |
| `DT_STRING` | `tf.string` | 字符串 |
| `DT_BOOL` | `tf.bool` | 布尔值 |

另外，每个张量拥有阶（Rank），这是其维度的数量。例如，以下张量（在 Python 中定义为列表）的阶为 2：

```
t = [[1,2,3]，[4,5,6]，[7,8,9]]
```

张量可以有任何阶。二阶张量通常被认为是矩阵，一阶张量将是向量。零阶被认为是标量值。

TensorFlow 文档使用三种类型的命名约定来描述张量的维度：形状（Shape），阶（Rank）和维数（Dimension Number）。下表显示了它们之间的关系，以便使跟踪 TensorFlow 文档更容易：


| 形状 | 阶 | 维数 |
| --- | --- | --- |
| `[]` | 0 | 0-D |
| `[D0]` | 1 | 1-D |
| `[D0, D1]` | 2 | 2-D |
| `[D0, D1, D2]` | 3 | 3-D |
| … | … | … |
| `[D0, D1, ... Dn]` | n | n-D |

这些张量可以通过一系列 TensorFlow 软件包提供的转换进行操作。 下面，我们将在下表中讨论其中的一些内容。

在本章中，我们将详细介绍其中一些内容。 可以在 TensorFlow 的官方网站 [18] 上找到完整的转换列表和详细信息。


| 操作 | 描述 |
| tf.shape | 获取张量的形状 |
| tf.size | 获取张量的大小 |
| tf.rank | 获取张量的阶 |
| tf.reshape | 改变张量的形状，保持包含相同的元素 |
| tf.squeeze | 删除大小为 1 的张量维度 |
| tf.expand_dims | 将维度插入张量 |
| tf.slice | 删除部分张量 |
| tf.split | 将张量沿一个维度划分为多个张量 |
| tf.tile | 将一个张量多次复制，并创建新的张量 |
| tf.concat | 在一个维度上连接张量 |
| tf.reverse | 反转张量的特定维度 |
| tf.transpose | 转置张量中的维度 |
| tf.gather | 根据索引收集部分 |

例如，假设您要将`2×2000`（2D 张量）的数组扩展为立方体（3D 张量）。 我们可以使用`tf.expand_ dims`函数，它允许我们向张量插入一个维度：

```py
vectors = tf.constant(conjunto_puntos)
extended_vectors = tf.expand_dims(vectors, 0)
```

在这种情况下，`tf.expand_dims`将一个维度插入到由参数给定的一个张量中（维度从零开始）。

从视觉上看，上述转变如下：

![](https://jorditorres.org/wp-content/uploads/2016/02/image023.gif)

如您所见，我们现在有了 3D 张量，但我们无法根据函数参数确定新维度 D0 的大小。

如果我们使用`get_shape()`操作获得此`tensor`的形状，我们可以看到没有关联的大小：

```py
print expanded_vectors.get_shape()
```

它可能会显示：

```py
TensorShape([Dimension(1), Dimension(2000), Dimension(2)])
```

在本章的后面，我们将看到，由于 TensorFlow 形状广播， 张量的许多数学处理函数（如第一章所示），能够发现大小未指定的维度的大小，，并为其分配这个推导出的值。

### TensorFlow 中的数据存储

在介绍 TensorFlow 的软件包之后，从广义上讲，有三种主要方法可以在 TensorFlow 程序上获取数据：

1.  来自数据文件。
2.  数据作为常量或变量预加载。
3.  那些由 Python 代码提供的。

下面，我简要介绍其中的每一个。

1)  **数据文件**

通常，从数据文件加载初始数据。这个过程并不复杂，鉴于本书的介绍性质，我邀请读者访问TensorFlow 的网站 [19]，了解如何从不同文件类型加载数据。您还可以查看 Python 代码[`input_data.py`](https://github.com/jorditorresBCN/TutorialTensorFlow/blob/master/input_data.py) [20]（可在 Github 上找到），它从文件中加载 MNIST 数据（我将在下面几章使用它）。

2)  **变量和常量**

当谈到小集合时，也可以预先将数据加载到内存中；创建它们有两种基本方法，正如我们在前面的例子中看到的那样：

*   `constant(…)`用于常量
*   `Variable(…)`用于变量

TensorFlow 包提供可用于生成常量的不同操作。在下表中，您可以找到最重要的操作的摘要：


| 操作 | 描述 |
| --- | --- |
| `tf.zeros_like` | 创建一个张量，所有元素都初始化为 0 |
| `tf.ones_like` | 创建一个张量，所有元素都初始化为 1 |
| `tf.fill` | 创建一个张量，其中所有元素都初始化为由参数给出的标量值 |
| `tf.constant` | 使用参数列出的元素创建常量张量 |

在 TensorFlow 中，在模型的训练过程中，参数作为变量保存在存储器中。 创建变量时，可以使用由函数参数定义的张量作为初始值，该值可以是常量值或随机值。 TensorFlow 提供了一系列操作，可生成具有不同分布的随机张量：


| 操作 | 描述 |
| --- | --- |
| `tf.random_normal` | 具有正态分布的随机值 |
| `tf.truncated_normal` | 具有正态分布的随机值，但消除那些幅度大于标准差 2 倍的值 |
| `tf.random_uniform` | 具有均匀分布的随机值 |
| `tf.random_shuffle` | 在第一维中随机打乱张量元素 |
| `tf.set_random_seed` | 设置随机种子 |

一个重要的细节是，所有这些操作都需要特定形状的张量作为函数的参数，并且创建的变量具有相同的形状。 通常，变量具有固定的形状，但TensorFlow提供了在必要时对其进行重塑的机制。

使用变量时，必须在构造图之后，在使用`run()`函数执行任何操作之前显式初始化这些变量。 正如我们所看到的，为此可以使用`tf.initialize_all_variables()`。 通过 TensorFlow 的`tf.train.Saver()`类，可以在训练模型时和之后将变量保存到磁盘上，但是这个类超出了本书的范围。

3)  **由Python代码提供**

最后，我们可以使用我们所谓的“符号变量”或占位符来在程序执行期间操作数据。调用是`placeholder()`，参数为元素类型和张量形状，以及可选的名称。

从 Python 代码调用`Session.run()`或`Tensor.eval()`的同时，张量由`feed_dict`参数中指定的数据填充。回想第 1 章中的第一个代码：

```py
import tensorflow as tf
a = tf.placeholder("float")
b = tf.placeholder("float")
y = tf.mul(a, b)
sess = tf.Session()
print sess.run(y, feed_dict={a: 3, b: 3})
```

在最后一行代码中，调用`sess.run()`时，我们传递两个张量`a`和`b`的值到`feed_dict`参数。

通过张量的简要介绍，我希望从现在起读者可以毫不费力地读懂下面几章的代码。

