# 五、词嵌入

神经网络方法的一个主要组成部分是嵌入的使用 - 将每个特征表示为低维空间中的向量。 但向量从哪里来？ 本节将综述常用方法。

## 5.1 随机初始化

当有足够可用的监督训练数据时，可以将特征嵌入视为与其他模型参数：将嵌入向量初始化为随机值，并让网络训练过程将它们调整为“好的”向量。 随机初始化必须小心执行。 有效的 word2vec 实现（Mikolov 等，2013; Mikolov，Sutskever，Chen，Corrado 和 Dean，2013）使用的方法是，将单词向量初始化为范围 $[-\frac{1}{2d},\frac{1}{2d}]$ 其中`d`是维数。 另一种选择是使用 xavier 初始化（请参阅第 6.3 节），并使用均匀采样的值进行初始化，它来自 $[-\frac{\sqrt 6}{\sqrt d},\frac{\sqrt 6}{\sqrt d}]$。

实际上，人们通常会使用随机初始化方法，来初始化常见特征的嵌入向量，例如词性标签或单个字母，同时使用某种形式的监督式或无监督式预训练，来初始化潜在的罕见特征 ，如单个词的特征。 然后，预训练的向量可以在网络训练过程中被视为固定的，或者更通常，像随机初始化的向量一样对待，并进一步调整到手边的任务。

## 5.2 监督的任务特定的预训练

如果我们对任务 A 感兴趣，其中我们只有有限数量的标记数据（例如句法分析），但是有一个辅助任务 B（比如说词性标注），其中我们有更多的标记数据，我们可能需要预先训练我们的单词向量，使它们可以很好地预测任务 B ，然后使用训练好的向量来训练任务 A。这样，我们就可以利用我们为任务 B 准备的大量标记数据 。当训练任务 A 时，我们可以将预训练的向量视为固定的，或者将它们进一步调整以用于任务 A。另一个选项是针对两个目标联合训练，参见第 7 节来获得更多细节。


## 5.3 无监督的预训练

常见的情况是，我们没有这样的辅助任务，它拥有足够多的标记数据（或者我们想用更好的向量训练它，来启动这个辅助任务）。 在这种情况下，我们诉诸于“无监督”的方法，这些方法可以在大量未标记的文本上训练。 训练单词向量的技术基本上是监督学习的技术，但是我们并不监督我们关心的任务，而是从原始文本中创建实际上无限数量的监督训练实例，希望我们创建的任务能够匹配（ 或者足够接近）我们关心的最终任务。

无监督方法背后的关键思想是，人们希望“相似”单词具有相似的向量。尽管词汇相似性很难定义，并且通常非常依赖于任务，但目前的方法来自分布假设（Harris，1954），指出如果词语出现在相似的语境中，它们是相似的。不同的方法都创建了监督训练实例，其目标是从其上下文中预测单词，或从单词中预测上下文。

在大量未标记数据上训练词语嵌入的一个重要好处是，它为未出现在有监督训练集中的词提供了向量表示。理想情况下，这些单词的表示与训练集中出现的相关单词的表示相似，从而使模型能够更好地概括看不见的事件。因此期望的是，通过无监督算法学习的词向量之间的相似性，捕获相同方面的相似性，它对于执行网络的预期任务有用。

常见的无监督词嵌入算法包括 word2vec [13]（Mikolov 等，2013, 2013），GloVe（Pennington，Socher 和 Manning，2014）和 Collobert 和 Weston（2008, 2011）嵌入算法。 这些模型受神经网络启发，并基于随机梯度训练。 然而，它们与另一系列算法有着深厚的联系，这些算法在 NLP 和 IR 社区中演化，都基于矩阵分解（对于讨论，参见（Levy & Goldberg，2014b; Levy 等，2015））。

可以说，辅助问题的选择（基于什么样的上下文预测什么），比用于训练它们的学习方法，对结果向量影响更多。 因此，我们专注于可用的辅助问题的不同选择，并且略过训练方法的细节。 可以使用几个软件包来导出单词向量，包括 word2vec [14] 和 Gensim [15]，它们实现了基于上下文的带有词窗口的 word2vec 模型，word2vecf [16] 是 word2vec 的修改版本，允许使用任意上下文，GloVe [17] 实现 GloVe 模型。 许多预训练的单词向量也可以在网上下载。 

虽然超出了本教程的范围，但值得注意的是，除了用于初始化神经网络模型的词嵌入层之外，无监督训练算法导出的嵌入词在 NLP 中有广泛的应用。

## 5.4 训练目标

给定单词`w`及其上下文`c`，不同的算法会制定不同的辅助任务。 在所有情况下，每个单词都被表示为一个`d`维向量，被初始化为一个随机值。 训练模型来良好地执行辅助任务，产生良好的词嵌入，以将词与上下文相关联，进而将使相似词的嵌入向量彼此相似。 （Mikolov 等，2013; Mnih & Kavukcuoglu，2013）以及 GloVe（Pennington 等，2014）采用的语言建模启发式方法使用了辅助任务，其目标是预测给定的单词上下文。 这是在概率的建立中提出的，试图模拟条件概率`P(w | c)`。


