# 六、神经网络的训练

神经网络的训练是通过尝试使用基于梯度的方法，最小化训练集上的损失函数来完成的。 粗略地说，所有训练方法的原理都是重复计算数据集上的误差估计，计算误差的梯度，然后沿梯度方向移动参数。 如何计算误差估计，以及如何定义“在梯度方向上移动”，不同模型是不同的。 我们描述了基本算法，随机梯度下降（SGD），然后提到其他方法，带有用于进一步阅读的链接。 梯度计算是该方法的核心。 梯度可以使用计算图上的反向模式微分来有效自动计算 - 它是一种通用的算法框架，可以自动计算任何网络和损失函数的梯度。

## 6.1 随机梯度训练

训练神经网络的常用方法是使用随机梯度下降（SGD）算法（Bottou，2012; LeCun，Bottou，Orr，和 Muller，1998a）或其变体。SGD 是一种通用优化算法。 它接收参数为`θ`的损失函数`f`，和所需的输入输出。 然后，它尝试设置参数`θ`，使得相对于训练样本的`f`的损失很小。 该算法的工作原理如下：

![](img/algo-1.jpg)

该算法的目标是设置参数`θ`，来最小化训练集上的总损失$\sigma_{i=1}^n L(f(x_i; \theta), y_i)$。 它的工作原理是重复采样训练样本，并计算误差对于参数`θ`的梯度（第 7 行） - 假设输入和预期输出是固定的，并且损失被视为参数`θ`函数。然后在梯度方向上更新参数`θ`，通过学习速率`η`（第 8 行）来缩放。 对于设置学习率的进一步讨论，请参见第 6.3 节。

请注意，第 6 行中计算的误差基于单个训练示例，因此仅仅是对我们打算使其最小的，语料库范围的损失的粗略估计。 损失计算中的噪声可能导致梯度不准确。 减少这种噪声的常用方法是基于`m`个样本来估计误差和梯度。 这就上升为小批量 SGD 算法：

![](img/algo-2.jpg)

在第 6-9 行中，算法基于小批量估计语料库损失的梯度。 在循环之后，`g`包含梯度估计，并且参数`θ`朝着`g`更新。 小批量大小的大小可以从`m = 1`到`m = n`。 较高的值可以更好地估计语料库范围的梯度，而较小的值可以提供更多的更新，从而更快地收敛。 除了提高梯度估计的准确性之外，小批量算法有机会提高训练效率。对于适度大小的`m`，一些计算架构（即 GPU）可以高效并行实现行 6-9 中的计算。 学习率足够小的情况下，如果函数是凸的，SGD 可以保证收敛到全局最优。 然而，它也可以用于优化非凸函数，例如神经网络。 虽然不再保证找到全局最优，但据证明该算法是健壮的并且在实践中表现良好。

在训练神经网络时，参数化函数`f`是神经网络，参数`θ`是层间传递矩阵，偏置项，嵌入矩阵等等。梯度计算是 SGD 算法以及所有其他神经网络训练算法中的关键步骤。那么问题是，如何计算网络误差对参数的梯度。幸运的是，有一种简单的解法，形式为反向传播算法（Rumelhart，Hinton，&Williams，1986; Lecun，Bottou，Bengio，&Haffner，1998b）。反向传播算法是一个奇特的名称，指代使用链式法则计算复杂表达式的导数，同时缓存中间结果。更一般，爱说，反向传播算法是反向模式自动微分算法的特例（Neidinger，2010，Section 7），（Baydin，Pearlmutter，Radul，&Siskind，2015; Bengio，2012）。以下部分描述了反向模式自动微分算法，在计算图抽象的上下文中。

超越 SGD。虽然 SGD 算法可以并且通常确实产生了良好的结果，但也可以使用更先进的算法。 SGD + 动量（Polyak，1964）和 Nesterov 动量（Sutskever，Martens，Dahl，&Hinton，2013）算法是 SGD 的变体，其中累积先前的梯度并且影响当前的更新。 自适应学习算法包括 AdaGrad（Duchi，Hazan，&Singer，2011），AdaDelta（Zeiler，2012），RMSProp（Tieleman & Hinton，2012）和 Adam（Kingma & Ba，2014），旨在为每个小批量选择学习率， 有时基于每个坐标，学习速率调度的需要可能会减少。 对于这些算法的详细信息，请参阅原始论文或（Bengio等，2015，第 8.3 和 8.4 节）。 由于许多神经网络软件框架提供了这些算法的实现，因此尝试不同的变体很容易，有时候也是值得的。

## 6.2 计算图抽象

虽然可以手动计算网络的各种参数的梯度，并且在代码中实现它们，但是该过程是麻烦且容易出错的。 对于大多数情况，最好使用自动工具进行梯度计算（Bengio，2012）。 计算图抽象允许我们轻松地构造任意网络，评估给定输入的预测（正向传递），并计算任意标量损失相对于其参数（反向传递）的梯度。


计算图是任意数学计算的表示，如图。 它是有向无环图（DAG），其中节点对应于数学运算或变量（绑定），并且边对应于节点之间的中间值的流。 根据不同组件之间的依赖关系，图结构定义计算的顺序。 图是 DAG 而不是树，因为一个操作的结果可以是几个后续的输入。 考虑例如用于计算`(a ∗ b + 1) ∗ (a ∗ b + 2)`的图：

![](img/0-1.jpg)

a * b的计算是共享的。 我们将自己局限于计算图连通的情况。由于神经网络本质上是一个数学表达式，它可以表示为计算图。

例如，图 3a 给出了具有 softmax 输出变换的 1 层 MLP 的计算图。 在我们的表示法中，椭圆形节点表示数学运算或函数，阴影矩形节点表示参数（绑定变量）。 网络的输入被视为常量，并且在没有周围节点的情况下绘制。 输入和参数节点没有传入边，输出节点没有传出边。 每个节点的输出是一个矩阵，其维度在节点上方标明。

该图是不完整的：没有指定输入，我们无法计算输出。图 3b 显示了 MLP 的完整图，它将三个单词作为输入，并预测第三个单词的词性标签的分布。 此图可用于预测，但不能用于训练，因为输出是矢量（不是标量），图并没有考虑正确答案或损失项。 最后，3c 中的图显示了特定训练示例的计算图，其中输入是“嵌入”单词“the”，“black”，“dog”，预期输出是“NOUN”（其索引为 5）。

![](img/3.jpg)

图 3：MLP1 的计算图。（a）输入未绑定的图表。（b）带有具体输入的图表。（c）带有输入，预期输出和损失节点的图表。

一旦构建了图，就可以直接执行正向计算（计算结果）或反向计算（计算梯度），如下所示。 构建图表可能看起来令人生畏，但实际上使用专用软件库和 API 非常容易。

正向计算。正向传播计算图中节点的输出。 由于每个节点的输出，仅取决于它自身及其传入边，因此通过以拓扑顺序遍历节点，并在其前驱的输出已计算的情况下，计算每个节点的输出来计算所有节点的输出是很简单的。
